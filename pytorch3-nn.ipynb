{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST(\"\", train = True, download = True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test =  datasets.MNIST(\"\", train = False, download = True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size= 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(train, batch_size= 10, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784,64)# fc is fully connected\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim = 1)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "net = Net()\n",
    "print(net)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5160e-02, 4.8033e-01, 5.1215e-01, 9.9835e-01, 8.0400e-01, 5.6946e-01,\n",
       "         4.6884e-03, 1.5324e-01, 3.6373e-01, 7.3482e-01, 5.7010e-01, 6.5211e-01,\n",
       "         9.3239e-01, 6.8211e-01, 7.6332e-01, 8.2211e-01, 3.7656e-01, 7.0561e-01,\n",
       "         9.4108e-01, 1.4873e-01, 2.8745e-01, 8.9300e-01, 1.7473e-01, 4.0431e-01,\n",
       "         5.9835e-01, 6.8690e-01, 1.3741e-02, 6.5144e-01, 1.9525e-01, 1.7623e-01,\n",
       "         4.8626e-01, 8.6311e-01, 1.9035e-01, 7.6706e-01, 9.3983e-01, 6.8862e-01,\n",
       "         8.4841e-01, 8.6652e-01, 9.8221e-01, 9.0184e-01, 2.2329e-01, 2.3558e-01,\n",
       "         2.4607e-01, 1.7024e-01, 3.4858e-02, 9.7929e-01, 8.8088e-01, 2.4926e-01,\n",
       "         3.9379e-01, 2.3618e-01, 1.1617e-02, 7.4060e-01, 2.5358e-01, 7.8895e-01,\n",
       "         4.7743e-01, 6.3027e-01, 8.6936e-02, 7.5473e-01, 7.6984e-01, 1.6389e-01,\n",
       "         5.2979e-01, 7.8717e-01, 3.2457e-01, 2.4835e-01, 7.0774e-01, 1.5322e-01,\n",
       "         8.5776e-01, 4.9046e-01, 1.3789e-01, 1.6400e-01, 7.0841e-01, 4.4326e-01,\n",
       "         6.5946e-01, 9.3699e-01, 3.2638e-01, 7.7699e-01, 5.9757e-01, 6.7341e-01,\n",
       "         4.4119e-01, 8.0102e-02, 6.9794e-01, 2.3368e-02, 7.5964e-01, 2.3575e-02,\n",
       "         7.4712e-01, 5.4136e-01, 4.1517e-02, 2.4095e-01, 7.6932e-01, 2.6694e-01,\n",
       "         6.1503e-01, 4.8011e-01, 5.7828e-04, 3.3397e-02, 5.1534e-01, 6.9233e-01,\n",
       "         3.4990e-01, 3.7849e-01, 8.3745e-01, 8.7190e-01, 2.9882e-01, 3.9913e-01,\n",
       "         8.9922e-01, 4.9787e-01, 6.1807e-01, 8.8482e-01, 2.4030e-02, 4.3695e-01,\n",
       "         4.6162e-01, 7.1084e-01, 6.7291e-01, 5.6062e-01, 4.1193e-02, 8.4174e-01,\n",
       "         1.9654e-01, 3.3234e-02, 9.7224e-01, 5.7542e-01, 2.7023e-01, 4.4239e-01,\n",
       "         3.1492e-01, 5.1497e-01, 4.1697e-01, 3.5948e-01, 9.0674e-01, 1.5111e-01,\n",
       "         6.6479e-01, 9.5968e-01, 7.2915e-01, 2.1724e-01, 7.5666e-01, 2.3634e-01,\n",
       "         1.9353e-01, 7.7780e-02, 7.8429e-01, 1.7925e-01, 4.7795e-01, 4.4184e-01,\n",
       "         8.6746e-01, 9.3578e-01, 2.5553e-02, 2.0367e-01, 4.8081e-01, 7.9959e-01,\n",
       "         4.0613e-01, 8.1645e-01, 2.7616e-01, 9.3987e-01, 8.3144e-01, 8.2245e-01,\n",
       "         6.0435e-01, 3.3464e-01, 7.5851e-01, 1.1199e-01, 7.7382e-01, 2.5989e-01,\n",
       "         8.7214e-01, 1.0036e-01, 6.2653e-02, 4.2114e-01, 1.4987e-01, 6.9065e-01,\n",
       "         7.6734e-01, 8.9502e-01, 7.1076e-01, 7.4425e-02, 8.0781e-01, 4.6216e-01,\n",
       "         4.1575e-01, 8.8337e-01, 5.6126e-01, 9.3874e-01, 3.3798e-01, 5.5891e-01,\n",
       "         4.4310e-01, 1.9874e-01, 8.4543e-01, 7.4645e-01, 2.6945e-01, 9.0780e-01,\n",
       "         3.7948e-01, 1.2954e-01, 6.4233e-01, 4.0515e-01, 7.2701e-01, 8.4230e-01,\n",
       "         3.1172e-01, 6.7999e-01, 1.9929e-01, 5.1225e-01, 2.9359e-02, 5.6768e-01,\n",
       "         7.1153e-02, 8.5509e-02, 9.2954e-01, 3.8363e-01, 2.6948e-01, 2.7176e-02,\n",
       "         9.9933e-01, 4.3191e-01, 6.2439e-01, 7.6945e-01, 8.9149e-01, 4.5919e-01,\n",
       "         9.1325e-01, 4.9620e-01, 8.2104e-01, 1.0241e-01, 3.8027e-01, 3.1477e-01,\n",
       "         6.9302e-01, 6.7207e-01, 4.5731e-01, 9.4545e-01, 9.2634e-01, 5.6047e-01,\n",
       "         7.4548e-01, 9.2811e-01, 3.5910e-01, 1.1252e-01, 6.3254e-01, 3.4755e-01,\n",
       "         7.3104e-01, 5.2680e-01, 2.4770e-02, 6.5382e-01, 5.5848e-01, 4.3251e-01,\n",
       "         3.0985e-01, 9.9600e-02, 5.2500e-01, 5.1591e-01, 4.7307e-01, 9.7700e-01,\n",
       "         4.6766e-02, 9.3218e-01, 7.8358e-01, 4.9153e-01, 9.2047e-01, 2.1097e-03,\n",
       "         1.7211e-01, 9.1853e-01, 7.4539e-01, 4.9303e-01, 8.8340e-01, 1.7534e-01,\n",
       "         9.1956e-01, 9.6542e-02, 5.5456e-01, 7.5321e-02, 6.8951e-03, 2.5113e-01,\n",
       "         7.4189e-01, 7.8662e-01, 9.3320e-01, 2.1665e-01, 8.3956e-01, 4.7471e-01,\n",
       "         8.7139e-01, 6.3351e-01, 5.5882e-01, 8.3053e-01, 7.1179e-01, 7.0586e-01,\n",
       "         1.2704e-02, 5.2303e-02, 7.1838e-01, 2.2116e-01, 4.9316e-01, 9.5748e-01,\n",
       "         8.7947e-01, 5.2503e-01, 1.6835e-01, 8.7214e-02, 2.3120e-01, 8.0238e-01,\n",
       "         7.8675e-02, 1.9513e-01, 5.5154e-01, 5.4820e-02, 1.7827e-01, 6.3173e-01,\n",
       "         9.9328e-01, 6.4398e-01, 7.4465e-02, 3.4114e-01, 5.1002e-01, 7.2861e-02,\n",
       "         1.2511e-01, 3.6116e-01, 9.2614e-01, 8.9476e-01, 4.6517e-01, 9.8630e-01,\n",
       "         2.8153e-01, 8.1330e-01, 5.3346e-01, 2.0710e-01, 3.0337e-01, 4.9205e-01,\n",
       "         7.5386e-01, 2.9410e-01, 9.0700e-01, 1.0276e-01, 6.8951e-01, 2.9602e-02,\n",
       "         6.5286e-02, 6.1902e-02, 8.1149e-01, 9.8992e-01, 5.5230e-03, 1.7788e-01,\n",
       "         2.7595e-01, 3.5519e-01, 5.3999e-01, 8.2600e-01, 4.8876e-01, 4.8212e-01,\n",
       "         6.5620e-01, 8.4162e-01, 2.0017e-01, 8.7275e-01, 4.2346e-01, 2.5886e-01,\n",
       "         8.2820e-01, 6.1858e-01, 6.8498e-01, 2.8565e-01, 1.9077e-01, 8.0705e-02,\n",
       "         5.4727e-01, 8.2856e-01, 6.7458e-01, 2.1343e-01, 5.7765e-01, 3.9475e-01,\n",
       "         6.6553e-01, 9.4552e-01, 7.3104e-01, 9.2048e-01, 3.7182e-01, 3.3655e-02,\n",
       "         8.3701e-01, 7.9282e-01, 2.5534e-01, 9.0559e-02, 2.3927e-01, 1.2176e-02,\n",
       "         3.1178e-02, 3.9995e-01, 2.2543e-01, 6.8470e-01, 8.3526e-01, 1.0889e-01,\n",
       "         6.2859e-01, 3.8028e-01, 8.2826e-01, 2.1265e-01, 5.0838e-01, 5.8618e-01,\n",
       "         9.0060e-01, 3.4972e-01, 6.0896e-01, 3.5915e-01, 9.0514e-01, 6.8678e-01,\n",
       "         6.2965e-01, 1.3653e-01, 8.2126e-01, 3.1101e-01, 2.3729e-01, 9.0712e-01,\n",
       "         8.5919e-01, 9.1913e-01, 2.4845e-01, 7.4959e-01, 9.2542e-02, 5.2735e-01,\n",
       "         9.0544e-01, 1.3394e-01, 7.9687e-01, 8.8474e-01, 2.2300e-01, 9.1986e-01,\n",
       "         4.9253e-01, 6.0752e-01, 7.0008e-01, 8.1030e-01, 8.4174e-01, 4.0737e-01,\n",
       "         6.9483e-01, 1.7780e-01, 2.0427e-01, 5.0559e-01, 1.2774e-01, 6.5341e-01,\n",
       "         9.7277e-01, 4.2671e-01, 2.7969e-01, 2.8825e-02, 4.3259e-01, 3.3001e-01,\n",
       "         3.3636e-01, 5.0594e-01, 8.2051e-01, 2.2048e-01, 6.3471e-01, 8.5888e-01,\n",
       "         7.1578e-01, 3.8558e-02, 4.0623e-01, 6.1613e-01, 1.8562e-01, 9.0601e-01,\n",
       "         2.1562e-01, 2.9355e-01, 3.3065e-01, 4.6635e-01, 6.2352e-02, 2.7606e-01,\n",
       "         9.8421e-01, 6.3273e-03, 1.9398e-02, 9.1509e-01, 4.2326e-01, 2.0894e-01,\n",
       "         5.8620e-01, 9.4416e-01, 3.0679e-01, 6.2212e-01, 8.2873e-03, 6.9720e-01,\n",
       "         9.9703e-01, 8.6283e-01, 8.8719e-01, 5.8276e-01, 5.0263e-01, 1.3436e-01,\n",
       "         3.4728e-01, 1.7815e-01, 2.1233e-01, 8.9959e-01, 8.3211e-01, 9.7814e-01,\n",
       "         4.3355e-01, 7.3609e-01, 2.1218e-01, 8.7803e-01, 9.8910e-01, 8.8625e-01,\n",
       "         1.7395e-01, 5.5482e-01, 2.4934e-01, 4.9350e-01, 3.0299e-01, 4.8189e-01,\n",
       "         6.2849e-01, 5.6740e-01, 4.9864e-01, 8.7790e-01, 8.1694e-01, 4.6863e-01,\n",
       "         9.0372e-01, 3.1306e-01, 2.6182e-01, 4.7196e-02, 2.0934e-01, 4.6319e-01,\n",
       "         6.5863e-02, 8.0497e-01, 8.2877e-01, 7.2772e-01, 6.5535e-01, 8.8614e-01,\n",
       "         5.2191e-01, 4.0188e-01, 3.3772e-01, 2.7687e-01, 1.3534e-01, 7.5536e-01,\n",
       "         1.7646e-01, 1.9021e-02, 5.5817e-01, 2.9612e-01, 2.0720e-01, 4.8270e-03,\n",
       "         7.5672e-01, 3.6822e-01, 4.5443e-02, 7.4046e-01, 4.8131e-01, 4.6530e-01,\n",
       "         9.2844e-01, 1.9995e-01, 4.1603e-01, 2.3498e-01, 8.8684e-01, 6.2389e-01,\n",
       "         8.3035e-01, 4.0045e-01, 3.7981e-01, 4.3183e-01, 4.3370e-01, 6.1929e-01,\n",
       "         7.9548e-01, 4.2399e-01, 9.5281e-01, 6.1540e-01, 4.6303e-01, 3.3951e-01,\n",
       "         7.6254e-01, 2.0060e-01, 9.5301e-01, 9.1235e-01, 6.2155e-01, 5.0685e-01,\n",
       "         5.2217e-01, 8.0007e-01, 2.6634e-01, 5.9057e-01, 2.0740e-01, 4.8460e-01,\n",
       "         2.7316e-01, 4.1982e-02, 7.6003e-01, 5.4152e-01, 5.8321e-02, 9.0687e-01,\n",
       "         7.8871e-01, 6.6263e-01, 1.3834e-03, 1.2582e-02, 2.6261e-01, 3.6903e-01,\n",
       "         2.7474e-02, 8.0065e-01, 6.3387e-01, 6.1393e-01, 2.3049e-01, 3.5925e-01,\n",
       "         5.8324e-01, 8.7609e-01, 7.1375e-01, 9.6233e-01, 4.1859e-01, 8.6794e-01,\n",
       "         2.7218e-01, 7.3376e-01, 7.1463e-01, 7.5451e-01, 3.6454e-01, 8.3411e-02,\n",
       "         5.3919e-01, 3.2165e-01, 2.5208e-01, 9.9607e-01, 8.0715e-01, 6.7577e-01,\n",
       "         7.0675e-01, 6.9795e-01, 2.2374e-01, 3.4123e-01, 4.9264e-01, 8.8263e-01,\n",
       "         4.9226e-01, 8.1993e-01, 5.5141e-01, 3.3551e-01, 6.3876e-01, 9.8097e-01,\n",
       "         6.8147e-01, 6.3607e-02, 9.5064e-01, 4.5157e-01, 7.7881e-01, 5.9507e-01,\n",
       "         5.6232e-03, 1.7316e-01, 1.6184e-02, 8.3238e-01, 2.3498e-01, 3.6179e-01,\n",
       "         1.6920e-01, 3.7535e-01, 7.5615e-01, 6.2602e-01, 7.6892e-01, 1.1753e-01,\n",
       "         8.2567e-01, 2.4063e-01, 5.7030e-01, 6.7449e-01, 6.0114e-01, 5.4916e-01,\n",
       "         8.0450e-01, 6.9926e-01, 2.4970e-01, 3.5479e-01, 2.2914e-01, 9.0069e-01,\n",
       "         5.3286e-01, 2.3093e-02, 6.7984e-01, 9.1012e-01, 6.1005e-01, 4.4441e-02,\n",
       "         2.8653e-01, 5.3485e-01, 6.6947e-01, 6.7952e-01, 1.8285e-01, 1.3620e-01,\n",
       "         1.2767e-01, 6.5561e-01, 2.4764e-01, 6.7008e-01, 6.0225e-01, 6.2027e-01,\n",
       "         9.0349e-01, 6.3011e-02, 8.3581e-01, 4.5981e-01, 6.2466e-01, 8.8994e-01,\n",
       "         9.8502e-01, 5.1357e-01, 9.5351e-01, 6.9629e-01, 1.1384e-01, 6.7413e-01,\n",
       "         5.0103e-01, 7.4055e-02, 2.9182e-02, 1.3821e-01, 6.9577e-01, 6.7413e-01,\n",
       "         7.2295e-01, 9.3468e-01, 2.1073e-01, 2.6087e-03, 2.5529e-01, 2.5828e-01,\n",
       "         6.3917e-02, 6.1416e-02, 5.8309e-01, 7.4248e-01, 7.1521e-01, 7.5065e-01,\n",
       "         7.5827e-01, 5.9098e-01, 2.0561e-01, 2.4010e-02, 4.8728e-01, 3.1982e-01,\n",
       "         8.6976e-01, 3.8434e-01, 4.7022e-01, 2.9916e-01, 6.6792e-01, 2.3063e-01,\n",
       "         9.2127e-01, 4.1507e-01, 1.6503e-01, 9.9830e-01, 6.9667e-01, 6.9891e-01,\n",
       "         2.3309e-01, 8.2284e-01, 4.2822e-01, 4.8797e-02, 3.2468e-01, 3.0131e-01,\n",
       "         1.1306e-01, 7.9454e-01, 8.0815e-01, 7.0862e-01, 3.4406e-03, 3.8764e-01,\n",
       "         1.9121e-01, 9.1595e-01, 7.2181e-01, 4.3567e-01, 2.6837e-01, 8.4812e-01,\n",
       "         5.9619e-02, 4.3902e-01, 7.6145e-01, 2.2522e-01, 3.5739e-01, 8.4936e-01,\n",
       "         8.5882e-01, 9.7790e-01, 3.9724e-01, 7.9578e-01, 2.5015e-02, 4.8489e-02,\n",
       "         3.1860e-01, 5.7698e-01, 5.3406e-01, 7.3462e-02, 6.9308e-01, 6.9270e-02,\n",
       "         3.8088e-01, 9.2549e-01, 2.1458e-01, 2.1064e-01, 4.8918e-01, 4.3354e-01,\n",
       "         8.0645e-01, 3.6134e-01, 2.0814e-01, 4.4460e-01, 3.8731e-01, 5.8583e-01,\n",
       "         1.0487e-01, 7.6127e-01, 4.9088e-01, 4.4129e-01, 9.0726e-01, 3.4399e-01,\n",
       "         4.6863e-01, 5.6273e-01, 3.7957e-01, 2.8088e-01, 5.3704e-01, 9.6334e-01,\n",
       "         2.2863e-01, 9.7023e-01, 7.5129e-01, 8.5665e-01, 1.6710e-01, 9.9444e-01,\n",
       "         4.8411e-01, 8.2215e-01, 8.1902e-01, 8.8102e-01, 3.4788e-01, 5.7702e-01,\n",
       "         2.6493e-01, 6.4229e-01, 2.3008e-02, 4.3411e-01, 1.9860e-01, 6.6612e-01,\n",
       "         1.4832e-01, 2.7612e-01, 7.8160e-01, 9.6360e-01, 5.1901e-01, 8.0254e-01,\n",
       "         3.1246e-01, 3.9907e-01, 4.1894e-01, 2.3819e-01, 6.1854e-01, 7.1752e-01,\n",
       "         7.6785e-01, 8.9057e-01, 4.0003e-01, 2.7670e-01, 8.0000e-02, 9.8549e-01,\n",
       "         6.6599e-02, 2.8533e-01, 4.5185e-01, 4.8520e-01, 7.3918e-01, 5.3836e-03,\n",
       "         6.9505e-01, 7.1409e-01, 6.8590e-01, 3.6630e-02, 3.4683e-02, 1.1476e-01,\n",
       "         2.1078e-01, 5.5667e-01, 8.0870e-01, 7.9315e-01, 2.4325e-01, 8.9785e-01,\n",
       "         3.7041e-01, 9.5743e-01, 1.9167e-02, 1.8805e-01]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(1,28*28)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3192, -2.4110, -2.4151, -2.1682, -2.2570, -2.2495, -2.2272, -2.4674,\n",
       "         -2.3223, -2.2314]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0297, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1027, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0107, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # datat is a batch of featuresets and labels\n",
    "        X,y = data\n",
    "        #everytime before you pass data through your neural network\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y)\n",
    "        loss.backward()#backprop\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X,y =data\n",
    "        output = net(X.view(-1,784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Accuracy:\", round(correct/total,3))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[9].view(28,28))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6, grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[9].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda7d6b9abbb5264fd3ab4663ee91a88227"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
